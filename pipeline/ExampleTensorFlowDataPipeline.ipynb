{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Example TensorFlow Data Pipeline\n",
    "Jeremy Karnowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Connecting data with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As mentioned in this [TensorFlow How-To](https://www.tensorflow.org/how_tos/reading_data/), there are a variety of ways that you can input data into TensorFlow:\n",
    "* Pre-loaded data - all of your data in a variable (con: your data has to fit in memory)\n",
    "* Directly feed data into a tensor (part of your computation graph)\n",
    "    * Create placeholders and feed data into this (pro: it will throw an error if you don't feed it)\n",
    "* Have TensorFlow read from a file:\n",
    "    * CSV files - will read each entry and injest\n",
    "    * Fixed length binary files - specify the size of input and it will read it directly\n",
    "    * TFRecords - a specialized binary file that contains more information about each input\n",
    "    \n",
    "If you are using [Amazon's Web Services](http://aws.amazon.com/) or [Google's Cloud Platform](https://cloud.google.com/) (or handling your own server architectures), then there are also opportunities to incorporate larger data pipelines with Hadoop, Spark, or Flink (open source version of Google Dataflow) which can deal with batch or streaming data at scale and supply it to a TensorFlow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Specify the source of raw data and destination of TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# List the directory containing the raw data\n",
    "DATA_DIR = \"/.../\"\n",
    "TF_DATA_DIR = \"/.../\"\n",
    "\n",
    "# List out the various datasets to use\n",
    "TRAIN_FILE = 'TRAIN.tfrecords'\n",
    "VALIDATION_FILE = 'VALIDATION.tfrecords'\n",
    "TEST_FILE = 'TEST.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your data might have different formats (images of different sizes)\n",
    "# In this case, you might need to know the size of the images that fit the shape of the network input\n",
    "# You can use these numbers to modify the images to be cropped or padded to be properly insert into the network\n",
    "\n",
    "IMAGE_HEIGHT = 0\n",
    "IMAGE_WIDTH = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convert raw data into TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Grab all the filenames relevant to the problem at hand\n",
    "# e.g. but do your own preprocessing to get proper subset\n",
    "filenames = [DATA_DIR+x for x in DATA_DIR]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "tfrecords_filename = \"FILENAME\"  # e.g. train.tfrecords, validation.tfrecords, test.tfrecords\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter(tfrecords_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Iterate through the files and create a record for each in tfrecords file\n",
    "\n",
    "for img_path in filenames:\n",
    "    \n",
    "    # Grab data, here image data\n",
    "    img = np.array(Image.open(img_path))\n",
    "\n",
    "    # Grab relevant features, here relevant image features \n",
    "    # (esp if structure matters for type of network, e.g. convolutional)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    label = int(img_path[-5]) # Modify for your problem\n",
    "    \n",
    "    # Convert image data\n",
    "    img_raw = img.tostring()\n",
    "    \n",
    "    # Create an example that contains all relevant info about your example\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'height': _int64_feature(height),\n",
    "        'width': _int64_feature(width),\n",
    "        'image_raw': _bytes_feature(img_raw),\n",
    "        'label': _int64_feature(label)}))\n",
    "    \n",
    "    # Write everything to the binary file\n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convert TFRecords into data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create method to sample from TFRecord file\n",
    "When running the graph, this will produce an example and the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    \n",
    "    reader = tf.TFRecordReader()\n",
    "    \n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Unpack the features you created in the method to save to TFRecords\n",
    "    # This is for images, but the features will differ depending on your problem\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            # 'depth': tf.FixedLenFeature([], tf.int64),      # if you encoded layers, e.g. IR cameras\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64), \n",
    "            # Other features you encoded during encoding process\n",
    "        })\n",
    "\n",
    "    # Grab the image data that has a fixed length storage\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    \n",
    "    # Get image shape to reshape it\n",
    "    height = tf.cast(features['height'], tf.int32)\n",
    "    width = tf.cast(features['width'], tf.int32)\n",
    "    \n",
    "    ##### RESHAPE - if needed\n",
    "    \n",
    "    # To reshape the image as an image with 3 channels:\n",
    "    # image_shape = tf.pack([height, width, 3])\n",
    "    # image = tf.reshape(image, image_shape)\n",
    "\n",
    "    # To reshape the image as a vector - here we use all three channels\n",
    "    image.set_shape([IMAGE_HEIGHT*IMAGE_WIDTH*3]) # creates a vector from the images\n",
    "    \n",
    "    # Unclear what this is for at the moment\n",
    "    # image_size_const = tf.constant((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=tf.int32)\n",
    "    \n",
    "    \n",
    "    ##### TRANSFORMATIONS\n",
    "    \n",
    "    # If the image needs to be cropped or padded for the network\n",
    "    # resized_image = tf.image.resize_image_with_crop_or_pad(image=image,\n",
    "    #                                        target_height=IMAGE_HEIGHT,\n",
    "    #                                        target_width=IMAGE_WIDTH)\n",
    "\n",
    "    # My example images in TFRecords are in [0, 255] in all channels, \n",
    "    # so I transformed to [-0.5, 0.5]\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    ##### MODIFY LABELS if needed\n",
    "    \n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create a way to grab batch of samples from TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inputs(train, batch_size, num_epochs):\n",
    "\n",
    "    if not num_epochs: num_epochs = None\n",
    "        \n",
    "    # For a better way to handle command line arguments\n",
    "    #   \n",
    "    # filename = os.path.join(FLAGS.train_dir,\n",
    "    #                         TRAIN_FILE if train else VALIDATION_FILE)\n",
    "    \n",
    "    # Using predefined folder for data\n",
    "    filename = os.path.join(TF_DATA_DIR, TRAIN_FILE if train else VALIDATION_FILE)\n",
    "    \n",
    "    # Creates a TF specific way to read file from folder\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs)\n",
    "\n",
    "    # Create the way to sample one example\n",
    "    # Even when reading in multiple threads, share the filename queue.\n",
    "    image, label = read_and_decode(filename_queue)\n",
    "\n",
    "    # Shuffle the examples and collect them into batch_size batches.\n",
    "    # The capacity and min_after_dequeue will affect memory usage depending on size\n",
    "    # of your samples. Modify accordingly\n",
    "    images, labels = tf.train.shuffle_batch(\n",
    "        [image, label], batch_size=batch_size, num_threads=2,\n",
    "        capacity=2000,\n",
    "        # Ensures a minimum amount of shuffling of examples.\n",
    "        min_after_dequeue=1000)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing out data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "images_batch, labels_batch = inputs(train=True, batch_size=BATCH_SIZE, num_epochs=10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # The op for initializing the variables.\n",
    "    # For some reason when using the \"with tf.Session() as sess\" version, \n",
    "    # this needs to be in scope. Shouldn't be the case, though. Just a warning\n",
    "    init = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer()) # needed for epochs to work\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Needed for multiple threads pulling from TFRecord\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # Actually run the TensorFlow graph that pulls out the images\n",
    "    images, labels = sess.run([images_batch, labels_batch])\n",
    "    \n",
    "    # Check their shape - should be equivalent to batch size and shape of data\n",
    "    print(images.shape)\n",
    "    \n",
    "    # Cease the multiple threads doing one action\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    \n",
    "    # Can do things with the data (e.g. See shape of data or visualize images)\n",
    "    for i in xrange(BATCH_SIZE):\n",
    "    \n",
    "        # For each image, can do something\n",
    "        print(images[i,:].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Using data for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run training using data input and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create methods with TensorBoard or something else to monitor the training below\n",
    "\n",
    "# Set up graph pieces to load data\n",
    "\n",
    "# Set up graph pieces that use samples and labels to do prediction and get losses\n",
    "\n",
    "# Set up graph pieces that create training procedure with specific optimizer\n",
    "\n",
    "# Set up method that runs the whole graph set up above and logs peformance to see in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## References\n",
    "* http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/\n",
    "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py\n",
    "* https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "* https://github.com/mnuke/tf-slim-mnist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
