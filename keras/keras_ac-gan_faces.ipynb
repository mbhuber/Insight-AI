{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "file: mnist_acgan.py\n",
    "author: Luke de Oliveira (lukedeo@vaitech.io)\n",
    "\n",
    "Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the\n",
    "MNIST dataset. See https://arxiv.org/abs/1610.09585 for more details.\n",
    "\n",
    "You should start to see reasonable images after ~5 epochs, and good images\n",
    "by ~15 epochs. You should use a GPU, as the convolution-heavy operations are\n",
    "very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as\n",
    "the compilation time can be a blocker using Theano.\n",
    "\n",
    "Timings:\n",
    "\n",
    "Hardware           | Backend | Time / Epoch\n",
    "-------------------------------------------\n",
    " CPU               | TF      | 3 hrs\n",
    " Titan X (maxwell) | TF      | 4 min\n",
    " Titan X (maxwell) | TH      | 7 min\n",
    "\n",
    "Consult https://github.com/lukedeo/keras-acgan for more information and\n",
    "example output\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "import cPickle as pickle\n",
    "#import cPickle as Pickle\n",
    "\n",
    "from PIL import Image\n",
    "#from Pillow import Image\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.datasets import cifar10#mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, merge, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Convolution2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.generic_utils import Progbar\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "imaChan = 3 # image channels\n",
    "imageDim = 64 # image size \n",
    "numClass = 6 # number of classes, range = 0,...,numClass-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faces_wiki_crop_clean_128_images.pkl\r\n",
      "faces_wiki_crop_clean_128_labels.pkl\r\n",
      "faces_wiki_crop_clean_64_aligned_images.pkl\r\n",
      "faces_wiki_crop_clean_64_aligned_labels.pkl\r\n",
      "faces_wiki_crop_clean_64_images.pkl\r\n",
      "faces_wiki_crop_clean_64_labels.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    # Faces\n",
    "    with open('../data/faces_wiki_crop_clean_64_aligned_images.pkl', 'rb') as handle:\n",
    "        imaArr = pickle.load(handle)\n",
    "    with open('../data/faces_wiki_crop_clean_64_aligned_labels.pkl', 'rb') as handle:\n",
    "        imaLabels = pickle.load(handle)    \n",
    "    print([imaArr.shape,imaLabels.shape])\n",
    "    \n",
    "    # force it to be of shape (..., 1, 28, 28) with range [-1, 1]\n",
    "    imaArr = imaArr.transpose((0,3,1,2))\n",
    "    print([imaArr.shape,imaLabels.shape])\n",
    "    imaArr = (imaArr.astype(np.float32) - 127.5) / 127.5\n",
    "    \n",
    "    # split data\n",
    "    ntrain=22000\n",
    "\n",
    "    X_train, y_train = imaArr[:ntrain], imaLabels[:ntrain]\n",
    "    X_test, y_test = imaArr[ntrain:], imaLabels[ntrain:]\n",
    "    imaArr, imaLabels=0,0\n",
    "    # reduce dimension\n",
    "    y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "    #X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    #X_train = np.expand_dims(X_train, axis=1)\n",
    "\n",
    "    # reduce training set for testing\n",
    "    ntrain=1000\n",
    "    X_train, y_train = X_train[:ntrain], y_train[:ntrain]\n",
    "\n",
    "    #X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "    #X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "    # reduce test set for testing\n",
    "    ntest=100\n",
    "    X_test, y_test = X_test[:ntest], y_train[:ntest]\n",
    "\n",
    "    print([X_train.shape,y_train.shape])\n",
    "    print([X_test.shape,y_test.shape])\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23373, 64, 64, 3), (23373,)]\n",
      "[(23373, 3, 64, 64), (23373,)]\n",
      "[(1000, 3, 64, 64), (1000,)]\n",
      "[(100, 3, 64, 64), (100,)]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = getData()\n",
    "nb_train, nb_test = X_train.shape[0], X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_generator(latent_size):\n",
    "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
    "    # label drawn from P_c, to image space (..., 1, 28, 28)\n",
    "    \n",
    "    idim = imageDim\n",
    "    \n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Dense(1024, input_dim=latent_size, activation='relu'))\n",
    "    cnn.add(Dense(128 * idim/4 * idim/4, activation='relu'))\n",
    "    cnn.add(Reshape((128, idim/4, idim/4)))\n",
    "\n",
    "    # upsample to (..., idim/2, idim/2)\n",
    "    cnn.add(UpSampling2D(size=(2, 2)))\n",
    "    cnn.add(Convolution2D(256, 5, 5, border_mode='same',\n",
    "                          activation='relu', init='glorot_normal'))\n",
    "\n",
    "    # upsample to (..., idim, idim)\n",
    "    cnn.add(UpSampling2D(size=(2, 2)))\n",
    "    cnn.add(Convolution2D(128, 5, 5, border_mode='same',\n",
    "                          activation='relu', init='glorot_normal'))\n",
    "\n",
    "    # take a channel axis reduction\n",
    "    cnn.add(Convolution2D(imaChan, 2, 2, border_mode='same',\n",
    "                          activation='tanh', init='glorot_normal'))\n",
    "\n",
    "    # this is the z space commonly refered to in GAN papers\n",
    "    latent = Input(shape=(latent_size, ))\n",
    "\n",
    "    # this will be our label\n",
    "    image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    # number classes in age groups #MNIST\n",
    "    cls = Flatten()(Embedding(numClass, latent_size,\n",
    "                              init='glorot_normal')(image_class))\n",
    "\n",
    "    # hadamard product between z-space and a class conditional embedding\n",
    "    h = merge([latent, cls], mode='mul')\n",
    "\n",
    "    fake_image = cnn(h)\n",
    "\n",
    "    return Model(input=[latent, image_class], output=fake_image)\n",
    "\n",
    "def build_discriminator():\n",
    "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
    "    # the reference paper\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Convolution2D(32, 3, 3, border_mode='same', subsample=(2, 2),\n",
    "                          input_shape=(imaChan, imageDim, imageDim)))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Convolution2D(128, 3, 3, border_mode='same', subsample=(2, 2)))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Convolution2D(256, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "    cnn.add(LeakyReLU())\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "\n",
    "    image = Input(shape=(imaChan, imageDim, imageDim))\n",
    "\n",
    "    features = cnn(image)\n",
    "\n",
    "    # first output (name=generation) is whether or not the discriminator\n",
    "    # thinks the image that is being shown is fake, and the second output\n",
    "    # (name=auxiliary) is the class that the discriminator thinks the image\n",
    "    # belongs to.\n",
    "    fake = Dense(1, activation='sigmoid', name='generation')(features)\n",
    "    aux = Dense(10, activation='softmax', name='auxiliary')(features)\n",
    "\n",
    "    return Model(input=image, output=[fake, aux])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# batch and latent size taken from the paper\n",
    "nb_epochs = 1#50\n",
    "batch_size = 32#100\n",
    "latent_size = 100\n",
    "\n",
    "# Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
    "adam_lr = 0.0002\n",
    "adam_beta_1 = 0.5\n",
    "\n",
    "# build the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(\n",
    "    optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "    loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    ")\n",
    "\n",
    "# build the generator\n",
    "generator = build_generator(latent_size)\n",
    "generator.compile(optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "                  loss='binary_crossentropy')\n",
    "\n",
    "latent = Input(shape=(latent_size, ))\n",
    "image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "# get a fake image\n",
    "fake = generator([latent, image_class])\n",
    "\n",
    "# we only want to be able to train generation for the combined model\n",
    "discriminator.trainable = False\n",
    "fake, aux = discriminator(fake)\n",
    "combined = Model(input=[latent, image_class], output=[fake, aux])\n",
    "\n",
    "combined.compile(\n",
    "    optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "    loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1\n",
      "30/31 [============================>.] - ETA: 0s  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Testing for epoch 1:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.89 | 0.96            | 1.92 \n",
      "generator (test)       | 2.22 | 0.27            | 1.94 \n",
      "discriminator (train)  | 2.40 | 0.55            | 1.85 \n",
      "discriminator (test)   | 2.70 | 0.89            | 1.81 \n",
      "Elapsed time= 0.454834047953 min\n"
     ]
    }
   ],
   "source": [
    "train_history = defaultdict(list)\n",
    "test_history = defaultdict(list)\n",
    "\n",
    "ct = time.time()\n",
    "for epoch in range(nb_epochs):\n",
    "    print('Epoch {} of {}'.format(epoch + 1, nb_epochs))\n",
    "\n",
    "    nb_batches = int(X_train.shape[0] / batch_size)\n",
    "    progress_bar = Progbar(target=nb_batches)\n",
    "\n",
    "    epoch_gen_loss = []\n",
    "    epoch_disc_loss = []\n",
    "\n",
    "    for index in range(nb_batches):\n",
    "        progress_bar.update(index)\n",
    "        # generate a new batch of noise\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, latent_size))\n",
    "\n",
    "        # get a batch of real images\n",
    "        image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "        label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "        # sample some labels from p_c\n",
    "        sampled_labels = np.random.randint(0, numClass, batch_size)\n",
    "\n",
    "        # generate a batch of fake images, using the generated labels as a\n",
    "        # conditioner. We reshape the sampled labels to be\n",
    "        # (batch_size, 1) so that we can feed them into the embedding\n",
    "        # layer as a length one sequence\n",
    "        generated_images = generator.predict(\n",
    "            [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "        X = np.concatenate((image_batch, generated_images))\n",
    "        y = np.array([1] * batch_size + [0] * batch_size)\n",
    "        aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n",
    "\n",
    "        # see if the discriminator can figure itself out...\n",
    "        epoch_disc_loss.append(discriminator.train_on_batch(X, [y, aux_y]))\n",
    "\n",
    "        # make new noise. we generate 2 * batch size here such that we have\n",
    "        # the generator optimize over an identical number of images as the\n",
    "        # discriminator\n",
    "        noise = np.random.uniform(-1, 1, (2 * batch_size, latent_size))\n",
    "        sampled_labels = np.random.randint(0, numClass, 2 * batch_size)\n",
    "\n",
    "        # we want to train the genrator to trick the discriminator\n",
    "        # For the generator, we want all the {fake, not-fake} labels to say\n",
    "        # not-fake\n",
    "        trick = np.ones(2 * batch_size)\n",
    "\n",
    "        epoch_gen_loss.append(combined.train_on_batch(\n",
    "            [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels]))\n",
    "\n",
    "    print('\\nTesting for epoch {}:'.format(epoch + 1))\n",
    "\n",
    "    # evaluate the testing loss here\n",
    "\n",
    "    # generate a new batch of noise\n",
    "    noise = np.random.uniform(-1, 1, (nb_test, latent_size))\n",
    "\n",
    "    # sample some labels from p_c and generate images from them\n",
    "    sampled_labels = np.random.randint(0, numClass, nb_test)\n",
    "    generated_images = generator.predict(\n",
    "        [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n",
    "\n",
    "    X = np.concatenate((X_test, generated_images))\n",
    "    y = np.array([1] * nb_test + [0] * nb_test)\n",
    "    aux_y = np.concatenate((y_test, sampled_labels), axis=0)\n",
    "\n",
    "    # see if the discriminator can figure itself out...\n",
    "    discriminator_test_loss = discriminator.evaluate(\n",
    "        X, [y, aux_y], verbose=False)\n",
    "\n",
    "    discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
    "\n",
    "    # make new noise\n",
    "    noise = np.random.uniform(-1, 1, (2 * nb_test, latent_size))\n",
    "    sampled_labels = np.random.randint(0, numClass, 2 * nb_test)\n",
    "\n",
    "    trick = np.ones(2 * nb_test)\n",
    "\n",
    "    generator_test_loss = combined.evaluate(\n",
    "        [noise, sampled_labels.reshape((-1, 1))],\n",
    "        [trick, sampled_labels], verbose=False)\n",
    "\n",
    "    generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
    "\n",
    "    # generate an epoch report on performance\n",
    "    train_history['generator'].append(generator_train_loss)\n",
    "    train_history['discriminator'].append(discriminator_train_loss)\n",
    "\n",
    "    test_history['generator'].append(generator_test_loss)\n",
    "    test_history['discriminator'].append(discriminator_test_loss)\n",
    "\n",
    "    print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
    "        'component', *discriminator.metrics_names))\n",
    "    print('-' * 65)\n",
    "\n",
    "    ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.2f} | {3:<5.2f}'\n",
    "    print(ROW_FMT.format('generator (train)',\n",
    "                         *train_history['generator'][-1]))\n",
    "    print(ROW_FMT.format('generator (test)',\n",
    "                         *test_history['generator'][-1]))\n",
    "    print(ROW_FMT.format('discriminator (train)',\n",
    "                         *train_history['discriminator'][-1]))\n",
    "    print(ROW_FMT.format('discriminator (test)',\n",
    "                         *test_history['discriminator'][-1]))\n",
    "\n",
    "    # save weights every epoch\n",
    "    generator.save_weights(\n",
    "        'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "    discriminator.save_weights(\n",
    "        'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "\n",
    "    # generate some digits to display\n",
    "    noise = np.random.uniform(-1, 1, (10*numClass, latent_size))\n",
    "\n",
    "    sampled_labels = np.array([\n",
    "        [i] * 10 for i in range(numClass)\n",
    "    ]).reshape(-1, 1)\n",
    "\n",
    "    # get a batch to display\n",
    "    generated_images = generator.predict(\n",
    "        [noise, sampled_labels], verbose=0)\n",
    "\n",
    "    # arrange them into a grid\n",
    "    if imaChan==1:\n",
    "        img = (np.concatenate([r.reshape(-1, imageDim)\n",
    "                               for r in np.split(generated_images, 10)\n",
    "                               ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n",
    "    else:\n",
    "        img = (np.concatenate([r.transpose(0,2,3,1).reshape(-1, imageDim,3)\n",
    "                           for r in np.split(generated_images, 10)\n",
    "                           ], axis=1) * 127.5 + 127.5).astype(np.uint8)\n",
    "        \n",
    "\n",
    "    Image.fromarray(img).save(\n",
    "        'plot_epoch_{0:03d}_generated.png'.format(epoch))\n",
    "\n",
    "    pickle.dump({'train': train_history, 'test': test_history},\n",
    "    open('acgan-history.pkl', 'wb'))\n",
    "\n",
    "print(\"Elapsed time= {} min\".format((time.time()-ct)/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "generated_images = generator.predict(\n",
    "            [noise, sampled_labels.reshape((-1, 1))], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 3, 64, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 640, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = (np.concatenate([r.transpose(0,2,3,1).reshape(-1, imageDim,3)\n",
    "                           for r in np.split(generated_images, 10)\n",
    "                           ], axis=1) * 127.5 + 127.5).astype(np.uint8)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.split(generated_images,6,axis=0)\n",
    "len(tmp)\n",
    "print(tmp[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1317a8b90>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb0yh/C8D3ND+EqLMaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15dd66e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(generated_images[1,0,:],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sampled_labels = np.random.randint(0, numClass, 2 * nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c42c1cd6cdd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 100)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.layers[5].input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
